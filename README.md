# ARB-Dropout: Adaptive Gradient-Based Dropout for Uncertainty Estimation

PyTorch implementation of ARB-Dropout, an adaptive dropout method that dynamically adjusts rates via gradient variance for efficient uncertainty estimation in deep neural networks.

## Key Features
- ğŸš€ **20-50x faster** than MC-Dropout at test time
- ğŸ“Š Adaptive dropout rates based on gradient variance
- ğŸ” Combined epistemic + aleatoric uncertainty estimation
- ğŸ† Benchmarked on CIFAR-10/100, SVHN, and STL-10

## Installation
```bash
git clone https://github.com/sameekshya1999/Adaptive-Dropout.git
cd Adaptive-Dropout
pip install -r requirements.txt
